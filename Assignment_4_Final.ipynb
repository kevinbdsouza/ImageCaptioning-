{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from IPython import display\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import json\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "For this assignment, you will reuse the dataset you downloaded in assignment 2. This dataset contains a very large set of images, approximately 80K training images and 100 validation images, with multiple tags for each image. However that data *lacks captions* for the images, which is **vital** for this assignment. To obtain the captions for this assignment, download a few data files as shown below and add them to your `data/annotations` folder from assignment 2.\n",
    "\n",
    "`wget https://s3-us-west-2.amazonaws.com/cpsc532l-data/a4_data.zip`\n",
    "\n",
    "Following the data downloading and unzipping, the code below loads in the data into memory accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       "  0.0039  0.0078  0.0039  ...   0.0471  0.0471  0.0314\n",
       "  0.0039  0.0039  0.0039  ...   0.0353  0.0353  0.0392\n",
       "  0.0039  0.0039  0.0039  ...   0.0392  0.0392  0.0510\n",
       "           ...             ⋱             ...          \n",
       "  0.7137  0.7294  0.7137  ...   0.1686  0.1843  0.1686\n",
       "  0.7059  0.6902  0.6863  ...   0.1765  0.1804  0.2039\n",
       "  0.6784  0.6667  0.6706  ...   0.1922  0.2157  0.2275\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       "  0.1490  0.1490  0.1412  ...   0.0039  0.0039  0.0039\n",
       "  0.1451  0.1412  0.1373  ...   0.0039  0.0039  0.0039\n",
       "  0.1412  0.1373  0.1373  ...   0.0039  0.0039  0.0039\n",
       "           ...             ⋱             ...          \n",
       "  0.4392  0.4667  0.4549  ...   0.2588  0.2745  0.2863\n",
       "  0.4353  0.4235  0.4196  ...   0.2745  0.2980  0.3137\n",
       "  0.4118  0.4000  0.4000  ...   0.3020  0.3176  0.3020\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       "  0.5294  0.5294  0.5294  ...   0.1451  0.1412  0.1333\n",
       "  0.5255  0.5333  0.5373  ...   0.1725  0.1451  0.1412\n",
       "  0.5373  0.5490  0.5451  ...   0.2314  0.1843  0.1608\n",
       "           ...             ⋱             ...          \n",
       "  0.0118  0.0078  0.0078  ...   0.5216  0.5294  0.5137\n",
       "  0.0078  0.0078  0.0118  ...   0.5098  0.5216  0.5216\n",
       "  0.0078  0.0118  0.0039  ...   0.5294  0.5255  0.4784\n",
       "[torch.cuda.FloatTensor of size 1x3x224x224 (GPU 0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a global transformer to appropriately scale images and subsequently convert them to a Tensor.\n",
    "img_size = 224\n",
    "loader = transforms.Compose([\n",
    "  transforms.Scale(img_size),\n",
    "  transforms.CenterCrop(img_size),\n",
    "  transforms.ToTensor(),\n",
    "]) \n",
    "def load_image(filename, volatile=False):\n",
    "    \"\"\"\n",
    "    Simple function to load and preprocess the image.\n",
    "\n",
    "    1. Open the image.\n",
    "    2. Scale/crop it and convert it to a float tensor.\n",
    "    3. Convert it to a variable (all inputs to PyTorch models must be variables).\n",
    "    4. Add another dimension to the start of the Tensor (b/c VGG expects a batch).\n",
    "    5. Move the variable onto the GPU.\n",
    "    \"\"\"\n",
    "    image = Image.open(filename).convert('RGB')\n",
    "    image_tensor = loader(image).float()\n",
    "    image_var = Variable(image_tensor, volatile=volatile).unsqueeze(0)\n",
    "    return image_var.cuda()\n",
    "\n",
    "load_image('data/train2014/COCO_train2014_000000000009.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load annotations file for the training images.\n",
    "mscoco_train = json.load(open('data/annotations/a4_data/train_captions.json'))\n",
    "train_ids = [entry['id'] for entry in mscoco_train['images']]\n",
    "train_id_to_file = {entry['id']: 'data/train2014/' + entry['file_name'] for entry in mscoco_train['images']}\n",
    "\n",
    "# Extract out the captions for the training images\n",
    "train_id_set = set(train_ids)\n",
    "train_id_to_captions = defaultdict(list)\n",
    "for entry in mscoco_train['annotations']:\n",
    "    if entry['image_id'] in train_id_set:\n",
    "        train_id_to_captions[entry['image_id']].append(entry['caption'])\n",
    "\n",
    "# Load annotations file for the validation images.\n",
    "mscoco_val = json.load(open('data/annotations/a4_data/val_captions.json'))\n",
    "val_ids = [entry['id'] for entry in mscoco_val['images']]\n",
    "val_id_to_file = {entry['id']: 'data/val2014/' + entry['file_name'] for entry in mscoco_val['images']}\n",
    "\n",
    "# Extract out the captions for the validation images\n",
    "val_id_set = set(val_ids)\n",
    "val_id_to_captions = defaultdict(list)\n",
    "for entry in mscoco_val['annotations']:\n",
    "    if entry['image_id'] in val_id_set:\n",
    "        val_id_to_captions[entry['image_id']].append(entry['caption'])\n",
    "\n",
    "# Load annotations file for the testing images\n",
    "mscoco_test = json.load(open('data/annotations/a4_data/test_captions.json'))\n",
    "test_ids = [entry['id'] for entry in mscoco_test['images']]\n",
    "test_id_to_file = {entry['id']: 'data/val2014/' + entry['file_name'] for entry in mscoco_test['images']}\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We do the same preprocessing done in assignment 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "sentences = [sentence for caption_set in train_id_to_captions.values() for sentence in caption_set]\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "# Create the vocabulary. Note that we add an <UNK> token to represent words not in our vocabulary.\n",
    "vocabularySize = 1000\n",
    "word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "vocabulary = [\"<UNK>\"] + [e[0] for e in word_counts.most_common(vocabularySize-1)]\n",
    "word2index = {word:index for index,word in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabularySize)\n",
    "\n",
    "# Build the word2vec embeddings\n",
    "wordEncodingSize = 300\n",
    "filtered_sentences = [[word for word in sentence if word in word2index] for sentence in sentences]\n",
    "w2v = Word2Vec(filtered_sentences, min_count=0, size=wordEncodingSize)\n",
    "w2v_embeddings = np.concatenate((np.zeros((1, wordEncodingSize)), w2v.wv.syn0))\n",
    "\n",
    "# Define the max sequence length to be the longest sentence in the training data. \n",
    "maxSequenceLength = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "def preprocess_numberize(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into list of numbers (denoting the index into the vocabulary).\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(sentence.lower())\n",
    "        \n",
    "    # Add the <SOS>/<EOS> tokens and numberize (all unknown words are represented as <UNK>).\n",
    "    tokenized = [\"<SOS>\"] + tokenized + [\"<EOS>\"]\n",
    "    numberized = [word2index.get(word, 0) for word in tokenized]\n",
    "    \n",
    "    return numberized\n",
    "\n",
    "def preprocess_one_hot(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of one-hot vectors.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    one_hot_embedded = one_hot_embeddings[numberized]\n",
    "    \n",
    "    return one_hot_embedded\n",
    "\n",
    "def preprocess_word2vec(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of word2vec embeddings.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    w2v_embedded = w2v_embeddings[numberized]\n",
    "    \n",
    "    return w2v_embedded\n",
    "\n",
    "def compute_bleu(reference_sentences, predicted_sentence):\n",
    "    \"\"\"\n",
    "    Given a list of reference sentences, and a predicted sentence, compute the BLEU similary between them.\n",
    "    \"\"\"\n",
    "    reference_tokenized = [word_tokenize(ref_sent.lower()) for ref_sent in reference_sentences]\n",
    "    predicted_tokenized = word_tokenize(predicted_sentence.lower())\n",
    "    return sentence_bleu(reference_tokenized, predicted_tokenized)\n",
    "\n",
    "#print(preprocess_one_hot(sentences[2]))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "cc = SmoothingFunction()\n",
    "\n",
    "def compute_bleu(reference_sentence, predicted_sentence):\n",
    "    \"\"\"\n",
    "    Given a list of reference sentences, and a predicted sentence, compute the BLEU similary between them.\n",
    "    \"\"\"\n",
    "    reference_tokenized = [\"<SOS>\"] + word_tokenize(reference_sentence.lower()) + [\"<EOS>\"] \n",
    "    predicted_tokenized = word_tokenize(predicted_sentence.lower())\n",
    "    return sentence_bleu(reference_tokenized, predicted_tokenized,smoothing_function=cc.method4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Image Encoder\n",
    "\n",
    "We load in the pre-trained VGG-16 model, and remove the final layer, as done in assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential (\n",
       "  (0): Linear (25088 -> 4096)\n",
       "  (1): ReLU (inplace)\n",
       "  (2): Dropout (p = 0.5)\n",
       "  (3): Linear (4096 -> 4096)\n",
       "  (4): ReLU (inplace)\n",
       "  (5): Dropout (p = 0.5)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "encoder = models.vgg16(pretrained=True).cuda()\n",
    "modified_classifier = nn.Sequential(*list(encoder.classifier.children())[:-1])\n",
    "\n",
    "encoder.train()\n",
    "modified_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv (\n",
       "  (out): Linear (4096 -> 512)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "encoder_hidden_size = 4096\n",
    "hidden_size = 512\n",
    "\n",
    "class conv(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(conv, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.out = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.out(input)\n",
    "        return output\n",
    "\n",
    "converter = conv(encoder_hidden_size, hidden_size).cuda() \n",
    "converter \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup a Language Decoder\n",
    "\n",
    "We're going to reuse our decoder from Assignment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderLSTM (\n",
       "  (lstm): LSTM(300, 512)\n",
       "  (out): Linear (512 -> 1000)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "encoder_hidden_size = 4096\n",
    "input_size = wordEncodingSize\n",
    "hidden_size = 512\n",
    "output_size = vocabularySize\n",
    "\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input,hidden,state):\n",
    "        output = F.relu(input)\n",
    "        output,(hidden,state) = self.lstm(output,(hidden,state))\n",
    "        output = self.out(output)\n",
    "        output = F.log_softmax(output.squeeze())\n",
    "        return output.unsqueeze(0),hidden,state\n",
    "\n",
    "decoder = DecoderLSTM(input_size, hidden_size, output_size).cuda() \n",
    "decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train encoder-decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "def _sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def compute_loss(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_seq(arr, length, pad_token):\n",
    "\n",
    "    if len(arr) == length:\n",
    "        return np.array(arr)\n",
    "    \n",
    "    return np.concatenate((arr, [pad_token]*(length - len(arr))))\n",
    "                 \n",
    "\n",
    "def create_training(start,end,set_batch):\n",
    "    \n",
    "\n",
    "    sentence_lens = [len(preprocess_numberize(sentence)) for sentence in set_batch] \n",
    "\n",
    "    sorted_indices = sorted(list(range(len(sentence_lens))), key=lambda i: sentence_lens[i], reverse=True)\n",
    "    set_batch = [set_batch[i] for i in sorted_indices if sentence_lens[i] > 0]\n",
    "    \n",
    "\n",
    "    train_id_batch = [train_id for train_id in train_ids[start:end]]\n",
    "    train_id_batch = [train_id_batch[i] for i in sorted_indices if sentence_lens[i] > 0]\n",
    "    training_input = [load_image(train_id_to_file[train_id]).squeeze() for train_id in train_id_batch]\n",
    "    training_input = torch.stack(training_input)\n",
    "    \n",
    "    sentence_lens = [sentence_lens[i] for i in sorted_indices if sentence_lens[i] > 0]   \n",
    "    max_len = max(sentence_lens)                            \n",
    "                         \n",
    "    # Preprocess all of the sentences in each batch\n",
    "    w2v_embedded_list = [preprocess_word2vec(sentence) for sentence in set_batch]\n",
    "    w2v_embedded_list_padded = [pad_seq(embed, max_len, np.zeros(wordEncodingSize)) \n",
    "                                        for embed in w2v_embedded_list]\n",
    "    numberized_list = [preprocess_numberize(sentence) for sentence in set_batch]\n",
    "    numberized_list_padded = [pad_seq(numb, max_len, 0).astype(torch.LongTensor) for numb in numberized_list]\n",
    "    \n",
    "    one_hot_embedded_list = [preprocess_one_hot(sentence) for sentence in set_batch]\n",
    "    one_hot_embedded_list_padded = [pad_seq(embed, max_len, np.zeros(vocabularySize)) \n",
    "                                        for embed in one_hot_embedded_list]\n",
    "    \n",
    "    one_hot_output = Variable(torch.FloatTensor(one_hot_embedded_list_padded)).cuda()\n",
    "    one_hot_output = one_hot_output.transpose(0, 1)\n",
    "\n",
    "                \n",
    "    w2v_input = Variable(torch.FloatTensor(w2v_embedded_list_padded)).cuda()    \n",
    "    training_output = Variable(torch.LongTensor(numberized_list_padded)).cuda()    \n",
    "    training_output = training_output.transpose(0, 1)\n",
    "    w2v_input = w2v_input.transpose(0, 1)\n",
    "\n",
    "    return training_input,training_output,w2v_input,one_hot_output,sentence_lens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(input_variable, \n",
    "          target_variable, \n",
    "          w2v_input, \n",
    "          one_hot_output,\n",
    "          encoder, \n",
    "          decoder, \n",
    "          decoder_optimizer, \n",
    "          converter_optimizer,\n",
    "          input_lens,\n",
    "          criterion, \n",
    "          embeddings=w2v_embeddings):\n",
    "    \n",
    "    decoder_optimizer.zero_grad()\n",
    "    converter_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    #print(target_length)\n",
    "\n",
    "    # Pass through the encoder\n",
    "    features_output = encoder.features(input_variable)\n",
    "    classifier_input = features_output.view(batch_size, -1)\n",
    "    encoder_output = modified_classifier(classifier_input)\n",
    "    encoder_output = converter(encoder_output).unsqueeze(0)\n",
    "    \n",
    "    #print(encoder_output)  \n",
    "    \n",
    "    \n",
    "    # Construct the decoder input (initially <SOS> for every batch)\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index[\"<SOS>\"]]\n",
    "                                                for i in range(w2v_input.size(1))]])).cuda()\n",
    "    #print(decoder_input)\n",
    "\n",
    "    #print(encoder_output)\n",
    "    decoder_hidden = encoder_output\n",
    "    decoder_state = encoder_output\n",
    "\n",
    "    # Prepare the results tensor\n",
    "    all_decoder_outputs = Variable(torch.zeros(*one_hot_output.size())).cuda()\n",
    "    all_decoder_outputs[0] = Variable(torch.FloatTensor([[one_hot_embeddings[word2index[\"<SOS>\"]]\n",
    "                                                for i in range(w2v_input.size(1))]])).cuda()\n",
    "        \n",
    "    # Iterate over the indices after the first.\n",
    "    for t in range(1,target_length):\n",
    "        decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,decoder_state)\n",
    "    \n",
    "        \n",
    "        if random.random() <= 0.9:\n",
    "            decoder_input = w2v_input[t].unsqueeze(0)\n",
    "        else:\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "                       \n",
    "            #Prepare the inputs\n",
    "            decoder_input = torch.stack([Variable(torch.FloatTensor(embeddings[ni])).cuda()\n",
    "                                         for ni in topi.squeeze()]).unsqueeze(0)\n",
    "        \n",
    "        #print(decoder_input)\n",
    "        #print(decoder_hidden)\n",
    "        #print(decoder_state)\n",
    "        # Save the decoder output\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "    \n",
    "        #print(all_decoder_outputs.transpose(0,1).contiguous())\n",
    "    \n",
    "    loss = compute_loss(all_decoder_outputs.transpose(0,1).contiguous(),\n",
    "                        target_variable.transpose(0,1).contiguous(),\n",
    "                    Variable(torch.LongTensor(input_lens)).cuda())\n",
    "    \n",
    "    loss.backward()    \n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 10.0) \n",
    "    decoder_optimizer.step()\n",
    "    converter_optimizer.step()\n",
    "\n",
    "    return loss.data[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.006761196804046631\n",
      "50 0.23840796203613282\n",
      "100 0.21213768920898438\n",
      "150 0.20291189098358153\n",
      "200 0.19630762882232666\n",
      "250 0.189369158411026\n",
      "300 0.1844574998855591\n",
      "350 0.17976785559654235\n",
      "400 0.17656983375549318\n",
      "450 0.17413294992446898\n",
      "500 0.1699452073097229\n",
      "550 0.16861181197166442\n",
      "600 0.1670388701915741\n",
      "650 0.16565209383964538\n",
      "700 0.16334830818176269\n",
      "750 0.16337445936203002\n",
      "800 0.16284609575271605\n",
      "850 0.16028103785514833\n",
      "900 0.15539328904151917\n",
      "950 0.15874498701095582\n",
      "1000 0.1547668893814087\n",
      "1050 0.15507685441970825\n",
      "1100 0.1545604338645935\n",
      "1150 0.1552305139541626\n",
      "1200 0.1535365128517151\n",
      "1250 0.1534611372947693\n",
      "1300 0.15185172510147094\n",
      "1350 0.15140251383781433\n",
      "1400 0.1508380982875824\n",
      "1450 0.15354797415733337\n",
      "1500 0.1496823037147522\n",
      "1550 0.14983204655647278\n",
      "1600 0.15068031125068665\n",
      "1650 0.14989451909065246\n",
      "1700 0.14834486274719239\n",
      "1750 0.14702813482284546\n",
      "1800 0.1477630047798157\n",
      "1850 0.14922148065567017\n",
      "1900 0.14582167086601258\n",
      "1950 0.14751230807304383\n",
      "2000 0.14695905299186707\n",
      "2050 0.14277237462997436\n",
      "2100 0.14599822549819946\n",
      "2150 0.14468578567504883\n",
      "2200 0.14491723732948303\n",
      "2250 0.14636991567611693\n",
      "2300 0.14516428146362303\n",
      "2350 0.1461394193649292\n",
      "2400 0.143527552318573\n",
      "2450 0.14727713437080384\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 20\n",
    "\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0001) \n",
    "converter_optimizer = torch.optim.Adam(converter.parameters(), lr=0.0001) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#len(train_id_to_file)\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(50000//batch_size):\n",
    "        \n",
    "        start_idx = i * batch_size % len(train_id_to_file)\n",
    "        \n",
    "        batch_sen = [[] for y in range(batch_size*5)] \n",
    "    \n",
    "        count=0\n",
    "        for train_id in train_ids[start_idx:start_idx + batch_size]:\n",
    "            for n in range(5):\n",
    "                batch_sen[count+(n*batch_size)].append(train_id_to_captions[train_id][n])\n",
    "            count+=1    \n",
    "        \n",
    "        for cap in range(5):\n",
    "            \n",
    "            set_batch = []\n",
    "            for batch in batch_sen[(cap*batch_size):(cap+1)*batch_size]: \n",
    "                set_batch.append(batch[0])\n",
    "        \n",
    "            training_input,training_output,w2v_input,one_hot_output,input_lens = create_training(\n",
    "                start_idx, start_idx + batch_size,set_batch)\n",
    "        \n",
    "            loss = train(training_input,\n",
    "                     training_output, \n",
    "                     w2v_input,\n",
    "                     one_hot_output,\n",
    "                     encoder,\n",
    "                     decoder, \n",
    "                     decoder_optimizer,\n",
    "                     converter_optimizer,\n",
    "                     input_lens,    \n",
    "                     criterion)\n",
    "            \n",
    "            total_loss+=loss\n",
    "        \n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(i,total_loss/5000)\n",
    "            total_loss = 0\n",
    "    \n",
    "\n",
    "torch.save(encoder.state_dict(), './encoder4.pth')\n",
    "torch.save(converter.state_dict(), './converter4.pth')\n",
    "torch.save(decoder.state_dict(), './decoder4.pth')\n",
    "print(\"training done\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MAP and Sampling Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer with batch size of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#val_images and ground truth captions \n",
    "decoder.load_state_dict(torch.load('./decoder4new.pth'))\n",
    "converter.load_state_dict(torch.load('./converter4new.pth'))\n",
    "encoder.load_state_dict(torch.load('./encoder4new.pth'))\n",
    "\n",
    "#encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a zebra standing in a field with a <UNK> . <EOS>\n",
      "A zebra in captivity grazing in its exhibit.\n",
      " \n",
      "a man is riding a snowboard on a beach . <EOS>\n",
      "a person and a dog are standing near some cliffs\n",
      " \n",
      "a zebra standing on a dirt <UNK> in a field . <EOS>\n",
      "A close up of a zebra foraging on some grass\n",
      " \n",
      "a man is a frisbee in a field . <EOS>\n",
      "A dog sitting on a grassy hillside by a path.\n",
      " \n",
      "a dog is <UNK> on a <UNK> <UNK> . <EOS>\n",
      "A small white dog sitting on the floor on top of a rug.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "def map_inference(input_variable, embeddings=w2v_embeddings, max_length=20):\n",
    "    \n",
    "    features_output = encoder.features(input_variable)\n",
    "    classifier_input = features_output.view(1, -1)\n",
    "    encoder_output = modified_classifier(classifier_input)\n",
    "    encoder_output = converter(encoder_output).unsqueeze(0)\n",
    "\n",
    "    #print(encoder_output)\n",
    "\n",
    "    # Construct the decoder input (initially <SOS> for every batch)\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index[\"<SOS>\"]]]])).cuda()\n",
    "    #print(decoder_input)\n",
    "    decoder_hidden = encoder_output\n",
    "    decoder_state = encoder_output\n",
    "    \n",
    "    # Iterate over the indices after the first.\n",
    "    decoder_outputs = []\n",
    "    for t in range(1,max_length):\n",
    "        decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,decoder_state)\n",
    "    \n",
    "        # Get the top result\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        decoder_outputs.append(ni)\n",
    "\n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "        \n",
    "        #Prepare the inputs\n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]])).cuda()\n",
    "\n",
    "    return ' '.join(vocabulary[i] for i in decoder_outputs)\n",
    "\n",
    "#send val images \n",
    "for val_id in val_ids[:5]:    \n",
    "    img_input = load_image(val_id_to_file[val_id])\n",
    "    caption = map_inference(img_input)\n",
    "    print(caption)\n",
    "    print(val_id_to_captions[val_id][0])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two zebra are standing in a field with one facing . <EOS>\n",
      "A zebra in captivity grazing in its exhibit.\n",
      " \n",
      "a teddy bear sitting on <UNK> next to a rock <EOS>\n",
      "a person and a dog are standing near some cliffs\n",
      " \n",
      "a zebra stands inside with <UNK> running in to the ground next to a fence . <EOS>\n",
      "A close up of a zebra foraging on some grass\n",
      " \n",
      "a cow laying on a park near a green apples <EOS>\n",
      "A dog sitting on a grassy hillside by a path.\n",
      " \n",
      "black dog and white dog resting in a room with a dog . <EOS>\n",
      "A small white dog sitting on the floor on top of a rug.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "def sample_inference(input_variable, embeddings=w2v_embeddings, max_length=20):\n",
    "    \n",
    "    features_output = encoder.features(input_variable)\n",
    "    classifier_input = features_output.view(1, -1)\n",
    "    encoder_output = modified_classifier(classifier_input)\n",
    "    encoder_output = converter(encoder_output).unsqueeze(0)\n",
    "    \n",
    "\n",
    "    # Construct the decoder input (initially <SOS> for every batch)\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index[\"<SOS>\"]]]])).cuda()\n",
    "    decoder_hidden = encoder_output\n",
    "    decoder_state = encoder_output\n",
    "    \n",
    "    # Iterate over the indices after the first.\n",
    "    decoder_outputs = []\n",
    "    for t in range(1,max_length):\n",
    "        decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,decoder_state)\n",
    "        probs = np.exp(decoder_output.data[0].cpu().numpy())\n",
    "        sample_sum = probs[0]\n",
    "        random_sample = random.random()\n",
    "        ni = 0\n",
    "        while sample_sum < random_sample:\n",
    "            ni += 1\n",
    "            sample_sum += probs[ni]\n",
    "            \n",
    "        decoder_outputs.append(ni)\n",
    "\n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "        \n",
    "        #Prepare the inputs\n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]])).cuda()\n",
    "\n",
    "    return ' '.join(vocabulary[i] for i in decoder_outputs)\n",
    "\n",
    "#send val images #send val images \n",
    "for val_id in val_ids[:5]:    \n",
    "    img_input = load_image(val_id_to_file[val_id])\n",
    "    caption = sample_inference(img_input)\n",
    "    print(caption)\n",
    "    print(val_id_to_captions[val_id][0])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate performance\n",
    "\n",
    "For validation images compute the average BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.258505524361466\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "total_bleu = 0\n",
    "for val_id in val_ids:\n",
    "    #load image\n",
    "    img_input = load_image(val_id_to_file[val_id])\n",
    "    predicted = \"<SOS>\" + map_inference(img_input)\n",
    "    \n",
    "    #load all captions \n",
    "    cap_set = [[] for x in range(5)]\n",
    "    for n in range(5):\n",
    "        cap_set[n].append(val_id_to_captions[val_id][n])\n",
    "    \n",
    "    temp_bleu = 0\n",
    "    for n in range(5):\n",
    "        sentence = cap_set[n][0]\n",
    "        bleu_score = compute_bleu(sentence, predicted)\n",
    "        temp_bleu += bleu_score\n",
    "    \n",
    "    total_bleu += temp_bleu/5\n",
    "    \n",
    "print(total_bleu/len(val_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The inference and the bleu score are not satisfactory. The training is done with 90% teacher forcing and only with 50K images in the training data (5 times for each image). Better results could be possible by playing around with the learning rate, teacher forcing ratio and increasing the number of samples trained on. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
